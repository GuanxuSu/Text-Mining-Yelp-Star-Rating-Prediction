{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Yelp Star Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "More than ever before, people’s decisions of what to eat or where to go are subject to other people’s opinions. The internet has become the ultimate trove of the opinions of many people. Today, websites like Yelp have turned to a vast database for places and restaurants that include reviews and opinions written by customers. This crowdsourcing method of extracting satisfaction has succeeded in providing different opinions about a certain restaurant.\n",
    "\n",
    "This project mainly aims at \n",
    "* Finding out what makes a review positive or negative \n",
    "* Predicting the ratings of reviews based on the text and a small set of relevent attributes\n",
    "\n",
    "From our research, we find that words with sentiment orientation play a key role in predicting star ratings. Moreover, it is easier to distinguish positive (3-5 stars) reviews from negative ones (1-2 star) than telling the specific star rating."
   ]
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation\n",
    "For our experiments, we used a subset of Yelp reviews, which includes about 1.5 million entries. These data provide useful information such as business profile, review text, date of review etc. And the businesses that are included in have at least 3 reviews older than 14 days. The data looks like:\n",
    "\n",
    "**Varaible**|Value|\n",
    ":-----:|:-----:\n",
    "**Stars**|5| \n",
    "**Name**|Red Lobster| \n",
    "**Text**|I had the opportunity to try out the red lobste my server a jeanne and she is amazing loved her and the great service| \n",
    "**Date**|2/6/2014| \n",
    "**City**|Mesa| \n",
    "** ...**|...|\n",
    "\n",
    "\n",
    "#### 2.1 Sampling\n",
    "After some preliminary exploration, we find that there're a lot more positive reviews (4-star and 5-star) than negative (<=3 star) reviews, as shown in Figure 1. To balance the class distribution as well as reducing sample size, we randomly selected 100,000 reviews from each star for further analysis.\n",
    "\n",
    "<img src=\"./image/uneven.png\" width = \"300\" height = \"200\" alt=\"uneven\" align=center />\n",
    "<h6 align=\"center\"> Fig. 1. Star rating distribution before sampling </h6>                               \n",
    "\n",
    "\n",
    "#### 2.2 Data preprocessing\n",
    "First, we cleaned the review texts, by removing punctuation, digits, extra whitespaces and transforming all cases to lower cases. Then we performed spelling correction and lemmatization. \n",
    "\n",
    "Next we would like to take context into consideration. It can be misleading if we only foucus on single word and ignore context. For example, after tokenization we may extract positive information from the sentence \"I don't like it and I'm not happy.\". Meanwhile, it can convey totally opposite sentiments depending on different context, such as \"pretty good\" and \"pretty bad\", \"go back\" and \"never go back\". Thus, we took two steps to avoid interpreting out of context.\n",
    "* Change negation format to reserve the negative meaning before tokenizing. Eg. change \"don't like\" to \"not_like\" and treat it as a single word.\n",
    "* Transform common phrases together into one single word. Eg. \"highly recommeded\" to \"highly_recommeded\", \"reasonable price\" to \"reasonable_price\".\n",
    "\n",
    "The whole process is shown in Figure 2.\n",
    "\n",
    "\n",
    "<img src=\"image/data_preprocess.png\" width = \"600\" height = \"600\" alt=\"preprocess\" align=center />\n",
    "<h6 align=\"center\"> Fig. 2.  Data Preprocess Flow </h6>                               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering\n",
    "There're 7 variables in the raw data set, such as business location, review date etc. However after further exploration, we decide to only use review text as predictor, because other varaibles don't contribute significantly to star rating, also in common sense the star rating of a review depends mostly on its content. Thus our feature engineering part is based on review text.\n",
    "\n",
    "The raw text contains millions of unique words, however not all of them are useful for predicting star rating. We want to select most useful words and create representative features based on selected words. The next two sections give a detailed explanation of words selection and feature creation.\n",
    "\n",
    "#### 3.1 Words Selection\n",
    "\n",
    "The first question is what are useful words? Intuitively, \"great\" and \"worst\" are useful cause the former tend to appear more in high star rating reviews while the latter tend to appear more in low ones, as an opposite, \"food\" and \"service\" are useless since they seem to appear equally in each star rating. More generally, useful words are those with a significantly larger likelihood of appearing in certain star rating level than other levels, thus they have high ability to distinguish between different star ratings. \n",
    "\n",
    "The second question is the converage percentage of useful words. If only 10% of reviews contain those words, our analysis may not generalize well. Thus during words selection we also keep an eye on converage percentage. \n",
    "\n",
    "The following sections introduce two metrics we use to evaluate how \"useful\" a word is, and our selection result.\n",
    "\n",
    "##### 3.1.1 Inverse Docunment Frequency (IDF)\n",
    "The Inverse Docunment Frequency of a word measures how often it appear in a collection of reviews. The formula is as follows:\n",
    "\n",
    "$$ IDF(word) = \\frac{N}{n} ,$$ \n",
    "\n",
    "where $N$ is the total number of reviews, $n$ is the number of reviews containing the specific word. The larger IDF is, the fewer reviews contain the specific word. For example, words like \"food\", \"menu\", \"chicken\" and \"restaurant\", etc, have low IDF. Words like \"thump\", \"cray\" and \"coho\", etc, have high IDF.\n",
    "\n",
    "##### 3.1.2 Diversity\n",
    "\n",
    "The Diversity of a word measures its ability to distinguish between different star ratings. It is the unevenness of frequency amoung different star ratings. The formula is: \n",
    "\n",
    "$$Div(word) = \\frac{Standard Deviation}{Mean} ,$$\n",
    "\n",
    "where Mean and Standard Deviation are in terms of the word's frequency distribution in each level of star ratings.\n",
    "\n",
    "<img src=\"image/diversity.png\" width = \"400\" height = \"300\" alt=\"diversity\" align=center />\n",
    "<h6 align=\"center\"> Fig. 3. Example of Diversity</h6>    \n",
    "\n",
    "\n",
    "The figure above shows an example of how diversity of each word is calculated. If a word has frequency, $f=f_1,f_2,f_3,f_4,f_5,$ inside star 1 to star 5, respectively. Then the Diversity of this word is:\n",
    "$Div=\\frac{\\sqrt{\\sum\\limits_{i=1}^5 (f_i-\\bar{f})^2}}{\\bar{f}}$. The word “minute” has higher frequency in negative reviews and lower frequency in positive reviews, hence it has a higher diversity than the word “food”, which has frequency very close in each star rating.\n",
    "\n",
    "For example, words like \"worst food poisoning\", \"worst customer service \" and \"minite\", etc, have high Diversity. Words like \"invite\", \"muscle\" and \"food\", etc, have low Diversity.\n",
    "\n",
    "##### 3.1.3 Words Selection Result\n",
    "\n",
    "After we get the two metrics in 3.1.1 and 3.1.2, we can draw the following plot. Each point represents a unique word. \n",
    "Synthesize the two questions mentioned above, we come up with the threshold indicated by the two red lines. The vertical line is $x=0.4$, the other line is $y=6+4.5x$. Those two lines split the figure into 4 parts. \n",
    "\n",
    "<img src=\"image/diversity_idf.png\" width = \"500\" height = \"300\" alt=\"2-step model\" align=center />\n",
    "<h6 align=\"center\"> Fig. 4. Diversity vs IDF</h6>    \n",
    "\n",
    "The words on the bottom left part, such as \"food\", are very common but don't contain much useful information. The words lying on the top left part, such as \"aunt\", are rare and meaningless. The top right words, such as \"tattoo\", contain some information but only appear in very few reviews. We want to find words with high ability to distinguish different star rating and appears in more reviews. Hence, we select the word on the bottom right part, such as \"amazing\", \"manager\", etc. This part contains 5814 unique words and 99.2% reviews contain those words.\n",
    "\n",
    "\n",
    "#### 3.2 Fearture Creation\n",
    "\n",
    "##### 3.2.1 Word Score\n",
    "\n",
    "To measure the polarity of a word, we introduce the word score. Word score is the average star (after mean-subtraction and standardization) of the reviews containing this word. There are two reasons for standardization:\n",
    "\n",
    "**1)** Mean-subtraction gives meaning for the sign of scores. Positive words would have positive sign, vice versa.\n",
    "\n",
    "**2)** Not all words are in our builded dictionary. It is reasonable to regard the new words as 0 or missing value after standardization.\n",
    "\n",
    "The left image is the histogram of all the words in our train set. As we can see, most of the words are concentrated around 0, which does not provide much information about interpreting review stars. However, after selecting the 5800 bottom right words mentioned before, we see an obvious bimodal distribution. Some of the words are negative while some are positive. The plots prove that the words selection procedure helps to filter out useless words.\n",
    "\n",
    "<img src=\"image/histograms.png\" width = \"600\" height = \"300\" alt=\"2-step model\" align=center />\n",
    "<h6 align=\"center\"> Fig. 5. Word score distribution before and after words selection</h6> \n",
    "\n",
    "##### 3.2.2 Extracted Features\n",
    "**Minimum, Average and Maximum Scores**\n",
    "\n",
    "Now that we have a score for each word, we are able to convert a review into an ordered score vector **Y**. Minimum value, mean value and maximum value can be extracted from **Y**.\n",
    "* All the three plots have a increasing trend \n",
    "* The average score can best distinguish star rating among the three quantities.\n",
    "* The minimum score can not well distinguish 4-star and 5-star, but has the ability to distinguish different stars in negative reviews.\n",
    "* The maximum score also has the ability to distinguish the star ratings, especially 3-star and 4-star.\n",
    "\n",
    "<img src=\"image/scores.png\" width = \"600\" height = \"400\" alt=\"2-step model\" align=center />\n",
    "<h6 align=\"center\"> Fig. 6. Boxplot of Min/Avg/Max scores under each star rating</h6> \n",
    "\n",
    "**Emotion Trend**\n",
    "\n",
    "The three features above do not take the order information into account. For example, consider the two reviews below:\n",
    "\n",
    "1) One star off because the <font color='blue'>**service is miss**</font> but their curry make up for everything else. Seriously <font color='red'>**the best curry**</font> in town.\n",
    "\n",
    "2) Ambience was <font color='red'>**awesome**</font> but service a <font color='blue'>**so terrible**</font>. We didn’t even get a chance to try anything except for our first round of drinks.\n",
    "\n",
    "The first review is a 4 star and the second review is a 2 star. They both say something positive and something negative. It seems that people still feel okay when they say negative words at the begining but turn to say positive words next. However, if people say something positive but then turn to say negative words, they may not be happy. The three features we extracted before won't work well in this case. \n",
    "\n",
    "We need to introduce a new feature, called emotion trend. We already have the score vector **Y**. Denote the index vector as **X**. Then the emotion trend is calculated as the slope, $\\beta$, of the regression line of **Y** vs **X**. We further take a power transformation $\\tilde\\beta=sign(\\beta)\\sqrt{|\\beta|}$.\n",
    "\n",
    "There is an increasing trend of slope as star rating goes up. Furthermore, there's a relatively large gap between star 2 and star 3. This phenomenon shows that emotion trend has the ability to distinguish the star rating, especially when a review seems to be neutral.\n",
    "\n",
    "<img src=\"image/root_slope.png\" width = \"400\" height = \"200\" alt=\"2-step model\" align=center />\n",
    "<h6 align=\"center\"> Fig. 7.Boxplot of emotion trend under each star rating</h6> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modeling\n",
    "\n",
    "#### 4.1 Model by extracted features\n",
    "Now we have four extracted features for model building. We find that minimum score, average score and maximum score has cubed relationship with star rating. Hence we add the value of second and third order power for them.\n",
    "\n",
    "To get a interpretable model, we choose classification and regression tree model for this part. The fitted model can be represented as the figure below. For example, if a review has average score less than -0.1, squared  average score greater than 1.1 and squared minimum score greater than 5.8, then the fitted star rating is 1.2.\n",
    "\n",
    "Every node in the tree uses the feature which could split the reviews in the way that star rating of the two groups are farthest apart. For example, the average score can best distinguish different star rating at the first node which split the reviews into positve and negative groups. In both groups, average score continues to play a role in the second node. After that, the maximum score begins to work, measuring how positive a review is given it's positive. Meanwhile, the minimum score also begins to work, measuring how negative a review is given it's negative. Whereas the slope begins to work when a review seems to be neutral based on the other three features.\n",
    "\n",
    "<img src=\"image/tree.png\" width = \"700\" height = \"300\" alt=\"2-step model\" align=center />\n",
    "<h6 align=\"center\"> Fig. 8.Regression tree of star rating </h6> \n",
    "\n",
    "#### 4.2 Bag of Words Model\n",
    "\n",
    "In order to predict the star ratings more accurately, we tried some machine learning methods using sparse matrix, where the i-th row represents the i-th review, and the j-th column represents the j-th token. The (i,j)-entry of this matrix represents\n",
    "the tf-idf quantity of the j-th token in the i-th review.\n",
    "\n",
    "After several tries, we find it more accurate to consider both one single word and two pairwise words in the model fitting. For example: **I like it** is transformed into **i**, **like**, **it**, **i like** and **like it**. The sparse matrices in this modeling procedure are all built on tf-idf method.\n",
    "\n",
    "The initial thought is to buid a one-step model directly using Support Vector Classifier, Linear Regression and Naive Bayes. Study in a deep-going way, we tried to first divide the star rating into positive rating(3-5 stars) and negative rating(1-2 star(s)), and build two seperate models. This is how our 2-step models build:\n",
    "\n",
    "<img src=\"image/2stepmodel.png\" width = \"300\" height = \"200\" alt=\"2-step model\" align=center />\n",
    "<h6 align=\"center\"> Fig. 9. Two-step model procedure</h6> \n",
    "\n",
    "Supprt Vector Classifier is a useful way in binary classification of negtive and possitive reviews, which achiecves an accuracy around 90%. Here we compare several models:\n",
    "\n",
    "|       | Model | RMSE |\n",
    "| ------| ------ | ------ |\n",
    "| 1-step model | SVC | 0.75 |\n",
    "| 1-step model | NB | 0.84 |\n",
    "| 2-step model | SVC+SVC | 0.716 |\n",
    "| 2-step model | **SVC+Linear** | **0.665** |\n",
    "\n",
    "In the SVC model, we set penalty C=0.2; In linear regression, we choose 20000 most frequent words and word combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conclusion\n",
    "\n",
    "#### 5.1 Thesis Statement\n",
    "* Words with sentiment orientation play a key role in predicting star ratings.\n",
    "* It is easier to distinguish positive (3-5 stars) reviews from negative ones (1-2 star) than telling the specific star rating.\n",
    "\n",
    "#### 5.2 Strengths and Weaknesses\n",
    "\n",
    "In this section we analyze our strengths and weaknesses during the whole analysis procedure.\n",
    "\n",
    "* **Data Preparation**\n",
    "\n",
    "    **Pros: **(1) Take context into consideration (2) Balanced the star distribution\n",
    "\n",
    "    **Cons: ** May lose information due to limited sample size\n",
    "\n",
    "\n",
    "* **Feature Extraction**\n",
    "\n",
    "    **Pros: ** (1) Interpretability (2) Get more insight of how review text contributes to its star rating\n",
    "\n",
    "    **Cons: **(1) Subjectivity of words selection\n",
    "\n",
    "\n",
    "* **Modeling**\n",
    "\n",
    "    **Pros: **Good performance on test data (2) Time efficient: about 10 min per round\n",
    "\n",
    "    **Cons: **(1) For decision tree model: not accurate, without consideration of higher order interaction (2) Some violation of assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Division of work**\n",
    "* Guanxu Su: Data Preprocessing; Text Cleaning; Feature Extraction\n",
    "* Shurong Gu: Model Building; Final Kaggle Prediction\n",
    "* Yuwei Sun: Parameter Optimization; Presentation Organization"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
